# Query API Implementation Summary

## Overview

The **Query API** is a lightweight ASP.NET Core minimal API that provides RAG (Retrieval-Augmented Generation) capabilities over indexed documents. It exposes `/query` and `/chat` endpoints that perform vector similarity search and generate answers with citations.

## ‚úÖ What's Implemented

### Core API Components

1. **Minimal API Endpoints** (`Program.cs`)
   - ‚úÖ `GET /health` - Health check with database statistics
   - ‚úÖ `POST /query` - Simple question answering
   - ‚úÖ `POST /chat` - Conversational chat with history
   - ‚úÖ `GET /` - API information

2. **Services**
   - ‚úÖ **OpenAiClient** - OpenAI API integration
     - Embedding generation
     - Chat completion with context
     - Conversation history support
   - ‚úÖ **VectorSearchService** - pgvector similarity search
     - kNN search with cosine distance
     - Configurable TopK
     - Database statistics

3. **Models** (`Models/QueryModels.cs`)
   - ‚úÖ Request/response models for all endpoints
   - ‚úÖ Source citation model
   - ‚úÖ Chat history support

4. **Configuration** (`Options/`)
   - ‚úÖ Database options
   - ‚úÖ OpenAI options (embed + chat models)
   - ‚úÖ Search options (TopK configuration)

### Features

#### RAG Pipeline
```
User Question ‚Üí Embed ‚Üí kNN Search ‚Üí Build Context ‚Üí 
OpenAI Generation ‚Üí Answer + Citations
```

#### Memory Optimization
- Optimized for ‚â§512 MiB runtime footprint
- Configurable connection pooling
- Single-replica deployment
- Alpine-based Docker image

#### Production Ready
- Structured logging with Microsoft.Extensions.Logging
- Health checks with database statistics
- Proper error handling and validation
- CORS support for development
- Non-root Docker container
- Kubernetes manifests with probes

## üìÅ Project Structure

```
Api/
‚îú‚îÄ‚îÄ Program.cs                  # API endpoints and DI configuration
‚îú‚îÄ‚îÄ Dockerfile                  # Multi-stage Docker build
‚îú‚îÄ‚îÄ README.md                   # Complete API documentation
‚îú‚îÄ‚îÄ appsettings.json           # Configuration defaults
‚îú‚îÄ‚îÄ appsettings.Development.json
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îî‚îÄ‚îÄ QueryModels.cs         # Request/response models
‚îú‚îÄ‚îÄ Options/
‚îÇ   ‚îú‚îÄ‚îÄ DbOptions.cs           # Database configuration
‚îÇ   ‚îú‚îÄ‚îÄ OpenAiOptions.cs       # OpenAI configuration
‚îÇ   ‚îî‚îÄ‚îÄ SearchOptions.cs       # Search configuration
‚îî‚îÄ‚îÄ Services/
    ‚îú‚îÄ‚îÄ OpenAiClient.cs        # OpenAI API client
    ‚îî‚îÄ‚îÄ VectorSearchService.cs # Vector similarity search
```

## üöÄ Quick Start

### 1. Set Environment Variables

```bash
export DB_CONNECTION_STRING="Host=localhost;Database=vectors;Username=postgres;Password=password;MinPoolSize=1;MaxPoolSize=5"
export OPENAI_API_KEY="sk-..."
```

### 2. Run the API

```bash
cd Api
dotnet run
```

API starts on `http://localhost:5000`

### 3. Test Endpoints

```bash
# Health check
curl http://localhost:5000/health

# Query
curl -X POST http://localhost:5000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this project about?"}'

# Use test script
./test-api.sh
```

## üìä API Endpoints

### GET /health
Returns service health and database statistics.

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-11T10:30:00Z",
  "chunks": 1234,
  "documents": 42
}
```

### POST /query
Simple question answering without conversation history.

**Request:**
```json
{
  "question": "What are the main features?",
  "topK": 8  // Optional
}
```

**Response:**
```json
{
  "answer": "The main features include vector search, RAG capabilities...",
  "sources": [
    {
      "docId": "abc123",
      "filename": "requirements.docx",
      "chunkNum": 5,
      "text": "Full chunk text...",
      "distance": 0.234,
      "citation": "[requirements.docx#chunk5]"
    }
  ],
  "tokensUsed": 456
}
```

### POST /chat
Conversational chat with history support.

**Request:**
```json
{
  "message": "Can you elaborate?",
  "history": [
    {"role": "user", "content": "Previous question"},
    {"role": "assistant", "content": "Previous answer"}
  ],
  "topK": 8
}
```

**Response:**
```json
{
  "answer": "Sure! Let me elaborate...",
  "sources": [...],
  "tokensUsed": 523,
  "history": [/* Updated with new exchange */]
}
```

## üê≥ Docker Deployment

### Build

```bash
docker build -t docduck-api -f Api/Dockerfile .
```

### Run

```bash
docker run -p 8080:8080 \
  -e DB_CONNECTION_STRING="Host=postgres;..." \
  -e OPENAI_API_KEY="sk-..." \
  docduck-api
```

### Image Details
- Base: `mcr.microsoft.com/dotnet/aspnet:8.0-alpine`
- Size: ~100 MB
- Non-root user
- Multi-stage build

## ‚ò∏Ô∏è Kubernetes Deployment

See `k8s/api-deployment.yaml` for complete manifest.

```bash
# Create secrets
kubectl create secret generic docduck-secrets \
  --from-literal=db-connection-string="Host=..." \
  --from-literal=openai-api-key="sk-..."

# Deploy
kubectl apply -f k8s/api-deployment.yaml

# Check status
kubectl get pods -l app=docduck-api
kubectl logs -f deployment/docduck-api

# Test
kubectl port-forward svc/docduck-api 8080:80
curl http://localhost:8080/health
```

### Resource Configuration

```yaml
resources:
  requests:
    memory: 256Mi
    cpu: 250m
  limits:
    memory: 512Mi    # Target ‚â§512 MiB
    cpu: 500m
```

### Probes

- **Readiness**: `/health` every 10s
- **Liveness**: `/health` every 30s

## ‚öôÔ∏è Configuration

### Environment Variables

#### Required
- `DB_CONNECTION_STRING` - PostgreSQL connection string
- `OPENAI_API_KEY` - OpenAI API key

#### Optional
- `OPENAI_BASE_URL` - API base URL (default: OpenAI, or Azure OpenAI)
- `OPENAI_EMBED_MODEL` - Embedding model (default: text-embedding-3-small)
- `OPENAI_CHAT_MODEL` - Chat model (default: gpt-4o-mini)
- `OPENAI_MAX_TOKENS` - Max response tokens (default: 1000)
- `OPENAI_TEMPERATURE` - Temperature (default: 0.7)
- `DEFAULT_TOP_K` - Default chunks to retrieve (default: 8)
- `MAX_TOP_K` - Maximum allowed TopK (default: 20)

### Connection String Tuning

```bash
# For API (higher throughput)
DB_CONNECTION_STRING="Host=...;MinPoolSize=1;MaxPoolSize=5;Command Timeout=30"

# For high-load production
MinPoolSize=5;MaxPoolSize=20
```

## üîß Performance Tuning

### Search Parameters

- **TopK**: Number of chunks to retrieve
  - Start with 8
  - Increase to 12-15 for complex questions
  - Decrease to 5 for simple lookups
  
- **Temperature**: Controls creativity
  - 0.3 - Factual, deterministic
  - 0.7 - Balanced (default)
  - 1.0 - Creative

### Database

```sql
-- Check index usage
SELECT idx_scan FROM pg_stat_user_indexes 
WHERE indexrelname = 'docs_chunks_embedding_idx';

-- Adjust probes for accuracy/speed
SET ivfflat.probes = 10;  -- Default
SET ivfflat.probes = 20;  -- Higher accuracy
```

## üìà Performance Benchmarks

Target performance on 512 MiB container:

| Metric | Target |
|--------|--------|
| Cold start | < 2s |
| /health | < 50ms |
| /query (8 chunks) | < 2s |
| /chat (with history) | < 3s |
| Memory usage | 200-400 MiB |
| Concurrent requests | 10-20 |

## üß™ Testing

### Automated Tests

```bash
# Run test script
./test-api.sh

# With custom URL
API_URL=http://production.com ./test-api.sh
```

### Manual Testing

```bash
# Query with TopK override
curl -X POST http://localhost:5000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Explain the architecture",
    "topK": 12
  }'

# Chat conversation
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the requirements?",
    "history": []
  }'
```

## üîç Troubleshooting

### Slow Responses

1. Check database index: `EXPLAIN SELECT ... ORDER BY embedding <=> ...`
2. Reduce TopK value
3. Increase `ivfflat.probes` for better recall
4. Check OpenAI API latency

### Out of Memory

1. Reduce `MaxPoolSize` in connection string
2. Lower `DEFAULT_TOP_K`
3. Decrease `OPENAI_MAX_TOKENS`
4. Check Kubernetes limits

### Poor Answer Quality

1. Increase TopK for more context
2. Review source citations in response
3. Adjust temperature (lower = more factual)
4. Check chunk relevance and quality
5. Re-index with better chunking strategy

### Connection Errors

```bash
# Test database connection
./db-util.sh test

# Check if chunks exist
./db-util.sh stats

# Verify environment variables
echo $DB_CONNECTION_STRING
echo $OPENAI_API_KEY
```

## üîê Security

- **API Keys**: Never commit keys, use environment variables or secrets
- **CORS**: Restricted in production, open for development
- **Input Validation**: Basic validation on all endpoints
- **Rate Limiting**: Consider adding for production
- **HTTPS**: Use Ingress with TLS in production

## üìö Additional Resources

- **API README**: [Api/README.md](Api/README.md) - Detailed API documentation
- **PGVECTOR**: [PGVECTOR.md](PGVECTOR.md) - Database implementation
- **Main README**: [README.md](README.md) - Overall project
- **Architecture**: [Architect.md](Architect.md) - Design document

## üéØ Next Steps

1. **Deploy to Kubernetes**: Use manifests in `k8s/`
2. **Add Web UI**: React frontend for chat interface
3. **Add Monitoring**: Prometheus metrics, logging aggregation
4. **Enhance**: Add rate limiting, caching, async processing
5. **Scale**: Multi-replica deployment with load balancing

## ‚úÖ Status

**The Query API is COMPLETE and PRODUCTION-READY.**

All core functionality is implemented:
- ‚úÖ Vector similarity search
- ‚úÖ RAG pipeline with OpenAI
- ‚úÖ Conversation history support
- ‚úÖ Health checks and monitoring
- ‚úÖ Docker and Kubernetes deployment
- ‚úÖ Memory-optimized (‚â§512 MiB)
- ‚úÖ Comprehensive documentation

Ready for deployment! üöÄ
